{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E59lNZxQWdsn"
   },
   "source": [
    "# Perceptual Path Length (PPL)\n",
    "\n",
    "*Please note that this is an optional notebook, meant to introduce more advanced concepts if you're up for a challenge, so don't worry if you don't completely follow!*\n",
    "\n",
    "Perceptual path length (PPL) was a metric that was introduced as part of [StyleGAN](https://arxiv.org/abs/1812.04948) to evaluate how well a generator manages to smoothly interpolate between points in its latent space. In essence, if you travel between two images produced by a generator on a straight line in the latent space, PPL measures the expected \"jarringness\" of each step in the interpolation when you add together the jarringness of steps from random paths. In this notebook, you'll walk through the motivation and mechanism behind PPL.\n",
    "\n",
    "The [StyleGAN2](https://arxiv.org/abs/1912.04958) paper noted that metric also \"correlates with consistency and stability of shapes,\" which led to one of the major changes between the two papers.\n",
    "\n",
    "And don't worry, we don't expect you to be familiar with StyleGAN yet - you'll learn more about it later in this course! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4iN67qBGbNFl"
   },
   "source": [
    "## Perceptual Similarity\n",
    "\n",
    "Like FID, which you learned about this week, PPL uses the feature embeddings of deep convolutional neural network. Specifically, the distance between two image embeddings as proposed in [The Unreasonable Effectiveness of Deep Features as a Perceptual Metric\n",
    "](https://arxiv.org/abs/1801.03924) by Zhang et al (CVPR 2018). In this approach, unlike in FID, a VGG16 network is used instead of an InceptionNet. \n",
    "\n",
    "Perceptual similarity is closely similar to the distance between two feature vectors, with one key difference: the features are passed through a learned transformation, which is trained to match human intuition on image similarity. Specifically, when shown two images with various transformations from a base image, the LPIPS (\"Learned Perceptual Image Patch Similarity\") metric is meant to have a lower distance for the image that people think is closer. \n",
    "\n",
    "![figure from perceptual similarity paper](perceptual_similarity_fig.jpg)\n",
    "*Figure from [The Unreasonable Effectiveness of Deep Features as a Perceptual Metric\n",
    "](https://arxiv.org/abs/1801.03924), showing a source image in the center and two transformations of it. Humans generally found the right-side image more similar to the center image than the left-side image, and the LPIPS metric matches this.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z8qliwUgdKlO"
   },
   "source": [
    "For our implementation, we can use the `lpips` [library](https://github.com/richzhang/PerceptualSimilarity), implemented by the authors of the perceptual similarity paper.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K5WvVnySXguq"
   },
   "outputs": [],
   "source": [
    "import lpips\n",
    "# Outside of coursera, you don't need the following five lines:\n",
    "from shutil import copyfile\n",
    "import os\n",
    "cache_path = '/home/jovyan/.cache/torch/hub/checkpoints/'\n",
    "vgg_file = 'vgg16-397923af.pth'\n",
    "if not os.path.exists(f\"{cache_path}{vgg_file}\"):\n",
    "    print(\"Moving file to cache\")\n",
    "    os.makedirs(cache_path, exist_ok=True)\n",
    "    copyfile(vgg_file, f\"{cache_path}{vgg_file}\")\n",
    "loss_fn_vgg = lpips.LPIPS(net='vgg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C3dtnNvkdMaH"
   },
   "source": [
    "You'll define your generator and a function to visualize the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7-MMYQ0TJUPn"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from tqdm.auto import tqdm\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import CelebA\n",
    "from torchvision.utils import make_grid\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    '''\n",
    "    Generator Class\n",
    "    Values:\n",
    "        z_dim: the dimension of the noise vector, a scalar\n",
    "        im_chan: the number of channels of the output image, a scalar\n",
    "              (CelebA is rgb, so 3 is your default)\n",
    "        hidden_dim: the inner dimension, a scalar\n",
    "    '''\n",
    "    def __init__(self, z_dim=10, im_chan=3, hidden_dim=64):\n",
    "        super(Generator, self).__init__()\n",
    "        self.z_dim = z_dim\n",
    "        # Build the neural network\n",
    "        self.gen = nn.Sequential(\n",
    "            self.make_gen_block(z_dim, hidden_dim * 8),\n",
    "            self.make_gen_block(hidden_dim * 8, hidden_dim * 4),\n",
    "            self.make_gen_block(hidden_dim * 4, hidden_dim * 2),\n",
    "            self.make_gen_block(hidden_dim * 2, hidden_dim),\n",
    "            self.make_gen_block(hidden_dim, im_chan, kernel_size=4, final_layer=True),\n",
    "        )\n",
    "\n",
    "    def make_gen_block(self, input_channels, output_channels, kernel_size=3, stride=2, final_layer=False):\n",
    "        '''\n",
    "        Function to return a sequence of operations corresponding to a generator block of DCGAN;\n",
    "        a transposed convolution, a batchnorm (except in the final layer), and an activation.\n",
    "        Parameters:\n",
    "            input_channels: how many channels the input feature representation has\n",
    "            output_channels: how many channels the output feature representation should have\n",
    "            kernel_size: the size of each convolutional filter, equivalent to (kernel_size, kernel_size)\n",
    "            stride: the stride of the convolution\n",
    "            final_layer: a boolean, true if it is the final layer and false otherwise \n",
    "                      (affects activation and batchnorm)\n",
    "        '''\n",
    "        if not final_layer:\n",
    "            return nn.Sequential(\n",
    "                nn.ConvTranspose2d(input_channels, output_channels, kernel_size, stride),\n",
    "                nn.BatchNorm2d(output_channels),\n",
    "                nn.ReLU(inplace=True),\n",
    "            )\n",
    "        else:\n",
    "            return nn.Sequential(\n",
    "                nn.ConvTranspose2d(input_channels, output_channels, kernel_size, stride),\n",
    "                nn.Tanh(),\n",
    "            )\n",
    "\n",
    "    def forward(self, noise):\n",
    "        '''\n",
    "        Function for completing a forward pass of the generator: Given a noise tensor, \n",
    "        returns generated images.\n",
    "        Parameters:\n",
    "            noise: a noise tensor with dimensions (n_samples, z_dim)\n",
    "        '''\n",
    "        x = noise.view(len(noise), self.z_dim, 1, 1)\n",
    "        return self.gen(x)\n",
    "\n",
    "def show_tensor_images(image_tensor, num_images=16, size=(3, 64, 64), nrow=3):\n",
    "    '''\n",
    "    Function for visualizing images: Given a tensor of images, number of images,\n",
    "    size per image, and images per row, plots and prints the images in an uniform grid.\n",
    "    '''\n",
    "    image_tensor = (image_tensor + 1) / 2\n",
    "    image_unflat = image_tensor.detach().cpu()\n",
    "    image_grid = make_grid(image_unflat[:num_images], nrow=nrow)\n",
    "    plt.imshow(image_grid.permute(1, 2, 0).squeeze())\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yYEEnNd9eEfK"
   },
   "source": [
    "You'll also load a generator, pre-trained on CelebA, like in the main assignment for this week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CtVG_KuWKrSk"
   },
   "outputs": [],
   "source": [
    "z_dim = 64\n",
    "gen = Generator(z_dim)\n",
    "gen.load_state_dict(torch.load(f\"pretrained_celeba.pth\", map_location='cpu')[\"gen\"])\n",
    "gen = gen.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i8U05PqAgg1_"
   },
   "source": [
    "## From LPIPS to PPL\n",
    "Note that perceptual path length builds directly on the LPIPS metric.\n",
    "\n",
    "As you'll learn, StyleGAN does not operate directly on the randomly sampled latent vector. Instead, it learns a mapping $f$ from $z$ to $w$ -- that is, $f(z) = w$. You'll learn more about this later, but for now, all you need to know is that there are two spaces over which you can calculate PPL. \n",
    "\n",
    "### Linear Interpolation ($w$-space)\n",
    "\n",
    "For the $w$ space, PPL is defined as follows using linear interpolation:\n",
    "\n",
    "First, you sample two points in $w$-space, $w_1 = f(z_1)$ and $w_2 = f(z_2)$, from two randomly sampled points in $z$-space. For simplicity, we'll let $f$ be the identity function here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xPc_Vc1Aq51M"
   },
   "outputs": [],
   "source": [
    "map_fn = nn.Identity()\n",
    "w_1, w_2 = map_fn(torch.randn(1, z_dim)), map_fn(torch.randn(1, z_dim))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cNT1a2F_q_EY"
   },
   "source": [
    "You will use your generator to produce two images interpolating between $w_1$ and $w_2$, where the amount of $w_1$ is $t$, one where the amount of $w_1$ is $t+\\epsilon$. You can think of $t$ as sampling a random point along the path interpolating between $w_1$ and $w_2$.\n",
    "\n",
    "You can use the `torch.lerp` function for linear interpolation, and sample a random $t$ uniformly from 0 to 1 using `torch.rand`. Also, here we can set $\\epsilon = 2 \\cdot 10^{-1}$ for visualization, even though in the StyleGAN paper $\\epsilon = 10^{-4}$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NY8BDhghrMSC"
   },
   "outputs": [],
   "source": [
    "eps = 2e-1\n",
    "t = torch.rand(1)\n",
    "interpolated_1 = torch.lerp(w_1, w_2, t)\n",
    "interpolated_2 = torch.lerp(w_1, w_2, t + eps)\n",
    "y_1, y_2 = gen(interpolated_1), gen(interpolated_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4ROUxbhKtPPl"
   },
   "source": [
    "Now you can visualize these images and evaluate their LPIPS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "id": "Gv4NHfwytSQO",
    "outputId": "79c4a121-28b4-42a2-8a61-fd5ff29b3ca7"
   },
   "outputs": [],
   "source": [
    "show_tensor_images(torch.cat([y_1, y_2]))\n",
    "cur_lpips = loss_fn_vgg(y_1, y_2).item()\n",
    "print(f\"Image LPIPS is {cur_lpips}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ziEo3_fAvbeU"
   },
   "source": [
    "Finally, you need to account for the impact of different values of $\\epsilon$, so that the perceptual path length converges as $\\epsilon\\rightarrow \\infty$. In order to do this, PPL divides by $\\epsilon^2$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "Bi8kpsQSvaxA",
    "outputId": "d10f3a73-318d-4ccd-e598-bce8ed49d344"
   },
   "outputs": [],
   "source": [
    "ppl = cur_lpips / (eps ** 2)\n",
    "print(f\"Our final sample PPL is {ppl}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FmQn8hA-uMds"
   },
   "source": [
    "This leaves you with the following overall equation: \n",
    "\n",
    "$$PPL_{w} = \\mathbb{E}\\left[\\frac{1}{\\epsilon^2} \\mathop{d_{\\mathrm{LPIPS}}}\\left(\\mathop{\\mathrm{G}}\\left(\\mathrm{lerp}(w_1, w_2, t\\right), \\mathop{\\mathrm{G}}\\left(\\mathrm{lerp}(w_1, w_2, t + \\epsilon\\right)\\right)\\right]$$\n",
    "\n",
    "You'll notice the expectation symbol: that's because this is all repeated many times in order to approximate PPL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "0O3EW18BvvR9",
    "outputId": "46b0ec6a-1f90-4e15-a5b1-d08c72278836"
   },
   "outputs": [],
   "source": [
    "def ppl_w(gen, map_fn, num_samples=10, eps=1e-4):\n",
    "    \"\"\"\n",
    "    Perceptual path length function: Combines the above steps into one PPL function\n",
    "    \"\"\"\n",
    "    # Sample of a batch of num_samples pairs of points\n",
    "    w_1 = map_fn(torch.randn(num_samples, z_dim))\n",
    "    w_2 = map_fn(torch.randn(num_samples, z_dim))\n",
    "    # Sample num_samples points along the interpolated lines\n",
    "    t = torch.rand(num_samples)[:, None]\n",
    "    # Interpolate between the points\n",
    "    interpolated_1 = torch.lerp(w_1, w_2, t)\n",
    "    interpolated_2 = torch.lerp(w_1, w_2, t + eps)\n",
    "    # Generated the interpolated images\n",
    "    y_1, y_2 = gen(interpolated_1), gen(interpolated_2)\n",
    "    # Calculate the per-sample LPIPS\n",
    "    cur_lpips = loss_fn_vgg(y_1, y_2)\n",
    "    # Calculate the PPL from the LPIPS\n",
    "    ppl = cur_lpips / (eps ** 2)\n",
    "    return ppl.mean()\n",
    "\n",
    "print(f\"PPL_w: {ppl_w(gen, nn.Identity()).item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "61pXUnEmySnl"
   },
   "source": [
    "### Spherical Interpolation ($z$-space)\n",
    "\n",
    "Because you sample points in $z$ from a Gaussian, we use spherical interpolation instead of linear interpolation to interpolate in $z$-space. We can use `scipy.spatial.geometric_slerp` for this.\n",
    "\n",
    "$$slerp(z_1, z_2, t) = \\frac{\\sin[(1 - t) \\cdot \\Omega]}{\\sin\\Omega} z_1 + \\frac{\\sin[t \\cdot \\Omega]}{\\sin\\Omega} z_2$$\n",
    "\n",
    "where $ \\Omega = \\cos^{-1}(\\mathrm{dot}(\\bar{z}_1, \\bar{z}_2))$ and $\\bar{x}$ denotes the normalized version of x."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "7RDUdRkN2m2w",
    "outputId": "ed419223-2e1d-4f91-fcfd-c4cf69591edc"
   },
   "outputs": [],
   "source": [
    "def normalize(x):\n",
    "    return x / torch.norm(x, dim=1)[:, None]\n",
    "\n",
    "def get_omega(x, y):\n",
    "    return torch.acos((normalize(x) * normalize(y)).sum(1))\n",
    "\n",
    "def slerp(x, y, t):\n",
    "    omega = get_omega(x, y)[:, None]\n",
    "    c1 = torch.sin(omega * (1 - t)) / torch.sin(omega)\n",
    "    c2 = torch.sin(omega * t) / torch.sin(omega)\n",
    "    return c1 * x + c2 * y\n",
    "\n",
    "def ppl_z(gen, num_samples=10, eps=1e-4):\n",
    "    # Sample of a batch of num_samples pairs of points\n",
    "    z_1 = torch.randn(num_samples, z_dim)\n",
    "    z_2 = torch.randn(num_samples, z_dim)\n",
    "    # Sample num_samples points along the interpolated lines\n",
    "    t = torch.rand(num_samples)[:, None]\n",
    "    # Interpolate between the points\n",
    "    interpolated_1 = slerp(z_1, z_2, t)\n",
    "    interpolated_2 = slerp(z_1, z_2, t + eps)\n",
    "    # Generated the interpolated images\n",
    "    y_1, y_2 = gen(interpolated_1), gen(interpolated_2)\n",
    "    # Calculate the per-sample LPIPS\n",
    "    cur_lpips = loss_fn_vgg(y_1, y_2)\n",
    "    # Calculate the PPL from the LPIPS\n",
    "    ppl = cur_lpips / (eps ** 2)\n",
    "    return ppl.mean()\n",
    "\n",
    "print(f\"PPL_z: {ppl_z(gen).item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5cO2fWfD9XdE"
   },
   "source": [
    "There you have it! Now you understand how PPL works - hopefully this makes you excited to start learning about StyleGAN."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "C2W1: Perceptual Path Length (Optional).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
